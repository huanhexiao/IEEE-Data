{"citations": ["6976737", "6732492", "5278570"], "references": [], "details": {"publisher": "TUP", "issue_date": "Feb. 2011", "doi": "10.1016/S1007-0214(11)70008-6", "title": "Mutual information is copula entropy", "abstract": "Mutual information (MI) is a basic concept in information theory. Therefore, estimates of the MI are fundamentally important in most information theory applications. This paper provides a new way of understanding and estimating the MI using the copula function. First, the entropy of the copula, named the copula entropy, is defined as a measure of the dependence uncertainty represented by the copula function and then the MI is shown to be equivalent to the negative copula entropy. With this equivalence, the MI can be estimated by first estimating the empirical copula and then estimating the entropy of the empirical copula. Thus, the MIestimate is an estimation of the entropy, which reduces the complexity and computational requirements. Tests show that the method is more effective than the traditional method.", "journal_title": "Tsinghua Science and Technology", "firstpage": "51", "volume": "16", "lastpage": "54", "date_publication": "Feb. 2011", "sponsor": "Tsinghua University Press (TUP)", "date": "Feb. 2011", "date_current_version": "Tue Jan 17 00:00:00 EST 2012", "issue": "1", "pages": "51 - 54"}, "authors": ["Jian Ma", "Zengqi Sun"], "keywords": ["Entropy", "Estimation", "Joints", "Measurement uncertainty", "Mutual information", "Random variables", "copula entropy", "empirical copula", "estimation", "mutual information", ""], "arnumber": "6077935"}